{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9469d2f-7712-46e1-9425-367356fb0fda",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3777d1-224f-4436-aac9-654bc97596e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a16efbc-8ca2-4cdb-824c-3db93c50c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "337222c3-9aad-48db-a549-7af3246d4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition   # used by a Node to decide whether to use a tool\n",
    "from langchain.agents import Tool # required to conver function to tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc45aa9c-3b11-4a27-8c5f-25b44b79c047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a3602-2e38-4c13-84c9-d4903e15bfec",
   "metadata": {},
   "source": [
    "### Test LangChain OpenAi Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a53c7d-cd95-4845-8cca-481236bf9d59",
   "metadata": {},
   "source": [
    "OPENAI API USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b24cdd-361e-4a8e-8392-c51bc41c7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an LLM instance\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# # Make a simple inference\n",
    "# user_prompt = \"\"\"\n",
    "# What is the capital of the US\n",
    "\n",
    "# \"\"\"\n",
    "# messages = [ HumanMessage(content=user_prompt), \n",
    "#              SystemMessage(content=\"Make sure to present your response in bullet point without markdown format\")\n",
    "#            ]\n",
    "\n",
    "# response = llm.invoke(messages)\n",
    "\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321db84-9010-4d3e-8fc6-0d33b0631eb4",
   "metadata": {},
   "source": [
    "GOOGLE GENERATIVE API USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45a5785-d101-44c7-a078-ca5ed71f0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a653021-ec0d-46d3-94a1-a427012ddb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the US is **Washington, D.C.**\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "What is the capital of the US\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [ HumanMessage(content=user_prompt), \n",
    "             SystemMessage(content=\"Make sure to present your response in bullet point without markdown format and extra character.\")\n",
    "           ]\n",
    "\n",
    "\n",
    "google_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    google_api_key=gemini_api_key\n",
    ")\n",
    "\n",
    "print(google_llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27723fd-a852-4411-b8a5-c47b167851d4",
   "metadata": {},
   "source": [
    "### Assistant Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d8d770-83dc-4ca0-8db3-f7740d849eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorResponse(BaseModel):\n",
    "    feedback: str = Field(description=\"critical feedback on the assistant response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria has been met\")# the users request\n",
    "    user_input: bool = Field(description=\"True if more input is needed from the user for more clarity of the LLM get stucked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac10bd6-03e8-4c4d-8e54-3a181d4bbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = ChatGoogleGenerativeAI( model=\"gemini-2.5-flash\", \n",
    "                                       temperature=0, \n",
    "                                       google_api_key=gemini_api_key)\n",
    "\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8bcfae-ab60-4ff5-af0d-94a81eb441e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluatorResponse(feedback='The model should directly answer the question about the capital of the US.', success_criteria_met=False, user_input=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_llm_with_output.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb99673-f4a7-4c44-9b1d-d74547f967ba",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe866ef5-50e1-4e6b-8040-59babe524f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_Id</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Gender</th>\n",
       "      <th>city_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268408</td>\n",
       "      <td>02-01-1970</td>\n",
       "      <td>M</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269696</td>\n",
       "      <td>07-01-1970</td>\n",
       "      <td>F</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268159</td>\n",
       "      <td>08-01-1970</td>\n",
       "      <td>F</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270181</td>\n",
       "      <td>10-01-1970</td>\n",
       "      <td>F</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268073</td>\n",
       "      <td>11-01-1970</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_Id         DOB Gender  city_code\n",
       "0       268408  02-01-1970      M        4.0\n",
       "1       269696  07-01-1970      F        8.0\n",
       "2       268159  08-01-1970      F        8.0\n",
       "3       270181  10-01-1970      F        2.0\n",
       "4       268073  11-01-1970      M        1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Customer.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd435ee-8095-46c8-bec5-34c1558ab797",
   "metadata": {},
   "source": [
    "### Customised Tools (Node)\n",
    "\n",
    "This tool can be used as a node in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8110f7de-02dc-4f41-9922-e70d61071585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5647 rows and 4 columns.\n",
      "\n",
      "Column Information:\n",
      "\n",
      "               dtype  missing_values  non_null_count\n",
      "customer_Id    int64               0            5647\n",
      "DOB           object               0            5647\n",
      "Gender        object               2            5645\n",
      "city_code    float64               2            5645\n",
      "\n",
      "\n",
      "Numeric Column Statistics:\n",
      "\n",
      "              count           mean          std       min       25%       50%       75%       max\n",
      "customer_Id  5647.0  271037.281034  2451.261711  266783.0  268912.0  271028.0  273180.0  275265.0\n",
      "city_code    5645.0       5.472631     2.859918       1.0       3.0       5.0       8.0      10.0\n",
      "\n",
      "\n",
      "Categorical Column Summary:\n",
      "\n",
      "       count unique         top  freq\n",
      "DOB     5647   4056  27-12-1988     7\n",
      "Gender  5645      2           M  2892\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_summary(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Returns a summary of the dataset including:\n",
    "    - Shape (rows, columns)\n",
    "    - Column names and data types\n",
    "    - Count of missing values per column\n",
    "    - Basic statistics for numeric columns\n",
    "    - Top unique values for categorical columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    summary = []\n",
    "    summary.append(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
    "\n",
    "    # Column info\n",
    "    col_info = pd.DataFrame({\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"missing_values\": df.isnull().sum(),\n",
    "        \"non_null_count\": df.notnull().sum()\n",
    "    })\n",
    "    summary.append(\"Column Information:\\n\")\n",
    "    summary.append(col_info.to_string())\n",
    "    summary.append(\"\\n\")\n",
    "\n",
    "    # Numeric stats\n",
    "    numeric_desc = df.describe(include=[float, int]).transpose()\n",
    "    summary.append(\"Numeric Column Statistics:\\n\")\n",
    "    summary.append(numeric_desc.to_string())\n",
    "    summary.append(\"\\n\")\n",
    "\n",
    "    # Categorical stats\n",
    "    cat_desc = df.describe(include=[object, \"category\"]).transpose()\n",
    "    if not cat_desc.empty:\n",
    "        summary.append(\"Categorical Column Summary:\\n\")\n",
    "        summary.append(cat_desc.to_string())\n",
    "        summary.append(\"\\n\")\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "print(data_summary(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbaa484-0a53-4fe4-8a11-16b826e5ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_summary = Tool(name=\"summarize_data\", \n",
    "                    func=data_summary, \n",
    "                    description=\"Returns the summary of uploaded data including statistics for numerical and non-numerical fields.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ce59a-766f-4e75-8ae9-142c9eefd39f",
   "metadata": {},
   "source": [
    "### Agents and List of Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea4f7ee-f653-4a9e-97ed-16d916e75711",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322ad053-4b88-4b72-80e6-d80cbca13581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_llm =  ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "google_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "# bind tools to LLM\n",
    "google_llm__with_tools = google_llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db549e-9fb6-4401-8138-20473e352d1d",
   "metadata": {},
   "source": [
    "### State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa6ef166-c893-49d4-a5fe-a3738faf4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # blueprint for a dictionary with a list of any object. But here I paln to use dictionary since it is for llm\n",
    "    # something like {\"role\": \"user\", \"content\": \"Capital of Nig?\"}\n",
    "    # The messages uses the reducer. \n",
    "    # The others are simply values that we overwrite with any state change based on the evaluator LLM.\n",
    "    messages: Annotated[list, add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73b3fa-f6ac-4ced-9589-ff8d0f5665e9",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c443b548-51f4-431e-ba91-fe4398b5f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(state: State) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Worker function that manages system instructions and invokes the LLM with tools.\n",
    "\n",
    "    This function ensures the assistant always has an up-to-date system message\n",
    "    guiding its behavior (e.g., role, success criteria, or feedback from prior attempts).\n",
    "    It updates or prepends a SystemMessage to the conversation history, sends the\n",
    "    updated messages to the LLM, and returns the assistant's latest response.\n",
    "\n",
    "    Workflow:\n",
    "        1. Construct a dynamic system message based on:\n",
    "            - The success criteria from state\n",
    "            - Optional feedback on previous work\n",
    "        2. Check if a SystemMessage already exists in state[\"messages\"]:\n",
    "            - If yes, update its content with the new system message\n",
    "            - If no, prepend a new SystemMessage to the messages list\n",
    "        3. Invoke the LLM (with tools enabled) on the updated message history\n",
    "        4. Return the new assistant response so it can be merged into state\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state containing conversation history, \n",
    "                       success criteria, and optional feedback.\n",
    "\n",
    "    Returns:\n",
    "        dict: A partial state update containing:\n",
    "            - \"messages\": A list with the assistant's latest response message.\n",
    "    \"\"\"\n",
    "    system_message = f\"\"\"\n",
    "            You are a data analyst assistant that can use tools to complete analysis tasks.\n",
    "            \n",
    "            You specialize in:\n",
    "            - Return summary of the dataset (Statistics, data description)\n",
    "            - Exploring datasets (especially CSV files provided by the user)\n",
    "            - Answering business or technical questions from the data\n",
    "            - Explaining insights clearly in plain language\n",
    "            - Asking clarifying questions if the task is ambiguous\n",
    "            \n",
    "            You keep working on a task until either:\n",
    "            1. You have a question or clarification for the user, OR\n",
    "            2. The success criteria is met.\n",
    "            \n",
    "            This is the success criteria:\n",
    "            {state['success_criteria']}\n",
    "\n",
    "            You should reply either with:\n",
    "            - A question for the user about this assignment (if more context or data is needed), OR\n",
    "            - Your final analysis/answer if you have enough information.\n",
    "            \n",
    "            If you have a question for the user, reply by clearly stating it. For example:\n",
    "            Question: please provide the CSV file containing sales data for analysis.\n",
    "            \n",
    "            If you have finished, reply only with the final analysis/answer â€” do not ask a question.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add rejection feedback (if any)\n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "            Previously, you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "            Here is the feedback on why this was rejected:\n",
    "            {state['feedback_on_work']}\n",
    "            With this feedback, please continue the assignment, ensuring that you meet the success criteria or ask a question to the user if \n",
    "            the data is missing.\n",
    "        \"\"\"\n",
    "    \n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "\n",
    "    # prepend system message if it does not already exists\n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    response = google_llm__with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1bb1477-e33d-4909-9c71-b7f8abe6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decides_if_tool(state: State):\n",
    "    \"\"\"\n",
    "    Router function that decides the next node in the graph \n",
    "    based on the assistant's most recent message.\n",
    "\n",
    "    The function inspects the last message in the conversation:\n",
    "      - If the message contains a tool call, the flow should continue \n",
    "        to the \"tools\" node for execution.\n",
    "      - Otherwise, the flow proceeds to the \"evaluator\" node to \n",
    "        check if the assistant's response meets the success criteria.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state containing the conversation \n",
    "                       history and other workflow metadata.\n",
    "\n",
    "    Returns:\n",
    "        str: The label of the next node to route to.\n",
    "             - \"tools\" if the last message requests a tool call\n",
    "             - \"evaluator\" if no tool call is present\n",
    "    \"\"\"\n",
    "    last_message =  state['messages'][-1]\n",
    "\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"evaluator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcd7de56-a662-4fa8-9ae4-7132650bbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tools use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation\n",
    "\n",
    "\n",
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "\n",
    "    system_message = \"\"\"\n",
    "        You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "        Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether \n",
    "        the success criteria has been met, and whether more input is needed from the user.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"\n",
    "        You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "        The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "        {format_conversation(state['messages'])} \n",
    "        \n",
    "        The success criteria for this assignment is:\n",
    "        {state['success_criteria']}\n",
    "        \n",
    "        And the final response from the Assistant that you are evaluating is:\n",
    "        {last_response}\n",
    "        \n",
    "        Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "        Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and \n",
    "        unable to answer without help.\n",
    "    \"\"\"\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "    new_state = {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": f\"Evaluator Feedback on this answer: {eval_result.feedback}\"}],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed\n",
    "    }\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7132699a-f767-41ef-b3df-77444c558095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decides_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe01e3-b559-4116-beac-c8dccc3fe276",
   "metadata": {},
   "source": [
    "### Build Graph\n",
    "* Compile \n",
    "* Connect nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "952bab04-4f59-4bb9-b0b0-474ce2d586a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ac0a166-3681-4c9c-ac14-3a6172f9dce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e214352110>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"assistant\", assistant)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "476af97a-0d52-4071-beca-df79b69fc172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Branch with name `decides_if_tool` already exists for node `assistant`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m graph_builder.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgraph_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_conditional_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecides_if_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevaluator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevaluator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m graph_builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m graph_builder.add_conditional_edges(\u001b[33m\"\u001b[39m\u001b[33mevaluator\u001b[39m\u001b[33m\"\u001b[39m, decides_based_on_evaluation, {\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEND\u001b[39m\u001b[33m\"\u001b[39m: END})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\New-Era-Projects\\Data-Analyst-Assistant-App\\.venv\\Lib\\site-packages\\langgraph\\graph\\state.py:644\u001b[39m, in \u001b[36mStateGraph.add_conditional_edges\u001b[39m\u001b[34m(self, source, path, path_map)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# validate the condition\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.branches[source]:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    645\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBranch with name `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` already exists for node `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    646\u001b[39m     )\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# save it\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.branches[source][name] = BranchSpec.from_path(path, path_map, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Branch with name `decides_if_tool` already exists for node `assistant`"
     ]
    }
   ],
   "source": [
    "graph_builder.add_edge(START, \"assistant\")\n",
    "graph_builder.add_conditional_edges(\"assistant\", decides_if_tool, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"assistant\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", decides_based_on_evaluation, {\"assistant\": \"assistant\", \"END\": END})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1932c2c8-ec9c-47ac-84a1-a26622ea8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c55be2-2d92-4e88-9df5-4a5f77daa669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-(DA Assistant)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
